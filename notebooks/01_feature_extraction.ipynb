{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Get Audio from Spotify ID:\n",
    "Below is an implementation that allows you to get an audio track as a numpy array from its spotify track ID.\n",
    "To clean up the project and prevent large storage size, uncomment the try block which cleans up the 'temp_audio.wav' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import yt_dlp\n",
    "import librosa\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import soundfile as sf\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_audio_array(track_id, duration=30, sr=44100, start_time=0):\n",
    "    \"\"\"\n",
    "    Get audio as numpy array from Spotify track ID using YouTube, starting from a specific time.\n",
    "    \n",
    "    Parameters:\n",
    "        track_id (str): Spotify track ID\n",
    "        duration (float): Duration of audio to extract in seconds (default: 30)\n",
    "        sr (int): Sample rate (default: 44100)\n",
    "        start_time (float): Start time in seconds (default: 0)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Audio array, or None if extraction fails\n",
    "    \"\"\"\n",
    "    # Initialize Spotify client\n",
    "    try:\n",
    "        auth_manager = SpotifyClientCredentials(\n",
    "            client_id=os.getenv(\"SPOTIFY_CLIENT_ID\"),\n",
    "            client_secret=os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n",
    "        )\n",
    "        sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "        \n",
    "        # Get track info from Spotify\n",
    "        track = sp.track(track_id)\n",
    "        track_name = track['name']\n",
    "        artist_name = track['artists'][0]['name']\n",
    "        logger.info(f\"Fetching: {track_name} by {artist_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching Spotify track {track_id}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # YouTube download options\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': 'temp_audio.%(ext)s',\n",
    "        'quiet': False,  # Set to False for debugging\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "        }],\n",
    "    }\n",
    "    \n",
    "    # Download audio from YouTube\n",
    "    search_query = f\"{track_name} {artist_name} official audio\"\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(f\"ytsearch1:{search_query}\", download=True)\n",
    "            if not info or 'entries' not in info or not info['entries']:\n",
    "                logger.error(f\"No valid YouTube video found for query: {search_query}\")\n",
    "                return None\n",
    "        \n",
    "        # Verify file exists\n",
    "        if not os.path.exists('temp_audio.wav'):\n",
    "            logger.error(\"Download succeeded but temp_audio.wav was not created\")\n",
    "            return None\n",
    "        \n",
    "        # Load audio with offset and duration\n",
    "        audio, original_sr = librosa.load(\n",
    "            'temp_audio.wav', \n",
    "            sr=sr, \n",
    "            mono=True, \n",
    "            offset=start_time, \n",
    "            duration=duration\n",
    "        )\n",
    "        \n",
    "        # Ensure exact duration by padding if necessary\n",
    "        target_samples = duration * sr\n",
    "        if len(audio) < target_samples:\n",
    "            audio = np.pad(audio, (0, target_samples - len(audio)))\n",
    "        elif len(audio) > target_samples:\n",
    "            audio = audio[:target_samples]  # Shouldn't happen with duration param, but included for safety\n",
    "            \n",
    "        logger.info(f\"Audio loaded: {len(audio)/sr:.2f}s, {sr}Hz, shape: {audio.shape}, start_time: {start_time}s\")\n",
    "        return audio\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading or processing audio for {track_id}: {e}\")\n",
    "        try:\n",
    "            os.remove('temp_audio.wav')\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "def get_track_name(track_id):\n",
    "    \"\"\"Helper function to get track name for filename\"\"\"\n",
    "    try:\n",
    "        auth_manager = SpotifyClientCredentials(\n",
    "            client_id=os.getenv(\"SPOTIFY_CLIENT_ID\"),\n",
    "            client_secret=os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n",
    "        )\n",
    "        sp = spotipy.Spotify(auth_manager=auth_manager)\n",
    "        track = sp.track(track_id)\n",
    "        track_name = track['name'].replace(' ', '_').replace('/', '_')  # Make filename safe\n",
    "        artist_name = track['artists'][0]['name'].replace(' ', '_').replace('/', '_')\n",
    "        return f\"{artist_name}_{track_name}\"\n",
    "    except:\n",
    "        return \"audio\"\n",
    "\n",
    "# Example usage\n",
    "track_id = \"1zV72LFybhhjlhGwXPDZc4\"  # Example Spotify track ID\n",
    "track_name = get_track_name(track_id)\n",
    "audio = get_audio_array(track_id, duration=30, sr=44100, start_time=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### DSP Pipeline\n",
    "\n",
    "##### 1. Convert to Mono\n",
    "The first step in the signal processing pipeline is to convert the audio to mono, which librosa does when we load the audio file into an array.\n",
    "\n",
    "##### 2. Harmonic and Percussive Source Separation (HPSS)\n",
    "Splitting the audio signal into its harmonic and percussive components allows for cleaner signals to be analyzed. For key estimation, only the harmonic content is used which prevents the percussive frequencies that do not correspond to the key of the audio from effecting the estimation. Similarly, for tempo estimation, only the percussive signal is analyzed as teh percussion tends to show clearer rhythmic patterns which the model will more easily be able to detect and ascribe a BPM label to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dsp(audio, sr=44100):\n",
    "    \"\"\"\n",
    "    Perform DSP analysis with proper time scaling and layout.\n",
    "    Returns harmonic and percussive audio arrays.\n",
    "    \"\"\"\n",
    "    # Verify audio properties\n",
    "    duration = len(audio) / sr\n",
    "    print(f\"Processing audio: {duration:.2f}s, {len(audio)} samples\")\n",
    "    \n",
    "    # Compute STFT with appropriate parameters\n",
    "    n_fft = 2048\n",
    "    hop_length = 512\n",
    "    \n",
    "    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # Harmonic-percussive separation with tuned parameters\n",
    "    harmonic, percussive = librosa.decompose.hpss(stft, kernel_size=(13, 31))\n",
    "    \n",
    "    # Reconstruct time-domain signals\n",
    "    harmonic_audio = librosa.istft(harmonic, hop_length=hop_length, length=len(audio))\n",
    "    percussive_audio = librosa.istft(percussive, hop_length=hop_length, length=len(audio))\n",
    "    \n",
    "    # Normalize for consistency (prevent clipping)\n",
    "    harmonic_audio = harmonic_audio / np.max(np.abs(harmonic_audio)) if np.max(np.abs(harmonic_audio)) != 0 else harmonic_audio\n",
    "    percussive_audio = percussive_audio / np.max(np.abs(percussive_audio)) if np.max(np.abs(percussive_audio)) != 0 else percussive_audio\n",
    "    \n",
    "    # Plot (existing code)\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    gs = plt.GridSpec(4, 2, figure=fig, height_ratios=[1, 1, 1, 0.1], hspace=0.3)\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    img1 = librosa.display.specshow(\n",
    "        librosa.amplitude_to_db(np.abs(stft), ref=np.max),\n",
    "        y_axis='log', x_axis='time', sr=sr, hop_length=hop_length, ax=ax1\n",
    "    )\n",
    "    ax1.set(title=f'Full Spectrogram ({duration:.1f}s)')\n",
    "    ax1.label_outer()\n",
    "    \n",
    "    # Harmonic spectrogram\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    img2 = librosa.display.specshow(\n",
    "        librosa.amplitude_to_db(np.abs(harmonic), ref=np.max(np.abs(stft))),\n",
    "        y_axis='log', x_axis='time', sr=sr, hop_length=hop_length, ax=ax2\n",
    "    )\n",
    "    ax2.set(title='Harmonic Component')\n",
    "    ax2.label_outer()\n",
    "    \n",
    "    # Percussive spectrogram\n",
    "    ax3 = fig.add_subplot(gs[2, :])\n",
    "    img3 = librosa.display.specshow(\n",
    "        librosa.amplitude_to_db(np.abs(percussive), ref=np.max(np.abs(stft))),\n",
    "        y_axis='log', x_axis='time', sr=sr, hop_length=hop_length, ax=ax3\n",
    "    )\n",
    "    ax3.set(title='Percussive Component')\n",
    "    ax3.set_xlabel('Time (seconds)')\n",
    "    \n",
    "    cax = fig.add_subplot(gs[3, :])\n",
    "    fig.colorbar(img1, cax=cax, orientation='horizontal', format='%+2.0f dB')\n",
    "    cax.set_xlabel('Power (dB)')\n",
    "    \n",
    "    plt.subplots_adjust(top=0.95, bottom=0.08, left=0.08, right=0.92, hspace=0.4)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"STFT shape: {stft.shape}\")\n",
    "    print(f\"Frequency bins: {stft.shape[0]}, Time frames: {stft.shape[1]}\")\n",
    "    \n",
    "    # Return the reconstructed audio\n",
    "    return harmonic_audio, percussive_audio\n",
    "\n",
    "def simple_separation_playback(audio, sr=44100, track_name=\"audio\"):\n",
    "    \"\"\"\n",
    "    Simple version for quick harmonic/percussive separation and playback\n",
    "    Saves files to project directory instead of embedding in notebook\n",
    "    \"\"\"\n",
    "    # STFT parameters\n",
    "    n_fft = 4096\n",
    "    hop_length = n_fft // 4\n",
    "    \n",
    "    # Compute STFT and separate\n",
    "    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    harmonic_stft, percussive_stft = librosa.decompose.hpss(stft)\n",
    "    \n",
    "    # Reconstruct audio\n",
    "    harmonic_audio = librosa.istft(harmonic_stft, hop_length=hop_length, length=len(audio))\n",
    "    percussive_audio = librosa.istft(percussive_stft, hop_length=hop_length, length=len(audio))\n",
    "    \n",
    "    # Normalize for playback\n",
    "    harmonic_audio = harmonic_audio / np.max(np.abs(harmonic_audio))\n",
    "    percussive_audio = percussive_audio / np.max(np.abs(percussive_audio))\n",
    "    \n",
    "    # Save to files in project directory\n",
    "    harmonic_filename = f\"harmonic_{track_name}.wav\"\n",
    "    percussive_filename = f\"percussive_{track_name}.wav\"\n",
    "    \n",
    "    sf.write(harmonic_filename, harmonic_audio, sr)\n",
    "    sf.write(percussive_filename, percussive_audio, sr)\n",
    "    \n",
    "    print(f\"Harmonic Component (melody) saved as: {harmonic_filename}\")\n",
    "    print(f\"Percussive Component (drums) saved as: {percussive_filename}\")\n",
    "    \n",
    "    # Still provide playback in notebook\n",
    "    print(\"\\nPlayback in notebook:\")\n",
    "    print(\"Harmonic Component (melody):\")\n",
    "    display(Audio(harmonic_audio, rate=sr))\n",
    "    \n",
    "    print(\"Percussive Component (drums):\")\n",
    "    display(Audio(percussive_audio, rate=sr))\n",
    "    \n",
    "    return harmonic_audio, percussive_audio\n",
    "\n",
    "if audio is not None:\n",
    "    harmonic_audio, percussive_audio = perform_dsp(audio, sr=44100)\n",
    "    harmonic, percussive = simple_separation_playback(audio, track_name=track_name) # comment this line to improve performance - this is just to check the hpss output\n",
    "        \n",
    "\n",
    "else:\n",
    "    print(\"Failed to get audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "##### 3. Constant Q-Transform Chroma Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_cqt_spec(audio, sr=44100):\n",
    "    \"\"\"\n",
    "    Perform DSP analysis with proper time scaling and layout\n",
    "    \"\"\"\n",
    "    \n",
    "    chroma_cqt = librosa.feature.chroma_cqt(y=audio, sr=sr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(chroma_cqt, x_axis='time', y_axis='chroma', cmap='coolwarm')\n",
    "    plt.colorbar(label='Intensity')\n",
    "    plt.title('Constant-Q Chroma Spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if harmonic_audio is not None:\n",
    "    chroma_cqt_spec(harmonic_audio, sr=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "##### 4. Pitch-Shifting Augmentation\n",
    "It will undoubtedly be the case that the dataset will not have equal representation of all keys, to mitigate this, we can artificially create more datapoints in less common keys by pitch shifting songs, while ensuring that the tempo information remains constant. **it will have to be the case that the whole usable dataset is parsed to run analystics of which keys are more prevelant so that more datapoints can be generated for underrepresnted keys (to be implemented)**. The following implementation outlines the pitch shifting technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic_audio_shifted = librosa.effects.pitch_shift(y=harmonic_audio, sr=44100, n_steps=6)\n",
    "chroma_cqt_spec(harmonic_audio_shifted, sr=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio, sr=44100, n_fft=4096, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extracts pitch and key-relevant DSP features from an audio signal.\n",
    "    Designed to be used on the harmonic component (after HPSS).\n",
    "    Returns a dictionary of feature arrays for supervised ML.\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio : np.ndarray\n",
    "        Mono audio signal (preferably harmonic component)\n",
    "    sr : int\n",
    "        Sample rate\n",
    "    n_fft : int\n",
    "        FFT size for analysis\n",
    "    hop_length : int\n",
    "        Hop length for STFT-based operations\n",
    "    Returns\n",
    "    -------\n",
    "    features : dict\n",
    "        {\n",
    "        \"chroma_mean\": (12,),\n",
    "        \"chroma_std\": (12,),\n",
    "        \"key_template_corr\": (24,),\n",
    "        \"pitch_hist\": (12,)\n",
    "        }\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    # --- Tuning estimation ---\n",
    "    tuning = librosa.estimate_tuning(y=audio, sr=sr, n_fft=n_fft)\n",
    "\n",
    "    # Compute STFT once and keep phase\n",
    "    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    S, phase = np.abs(stft), np.exp(1j * np.angle(stft))\n",
    "    \n",
    "    # HPSS decomposition\n",
    "    H, _ = librosa.decompose.hpss(S)\n",
    "\n",
    "    # Reconstruct harmonic audio with original phase\n",
    "    harmonic = librosa.istft(H * phase, hop_length=hop_length, length=len(audio))\n",
    "\n",
    "    # --- Chroma CQT (tuning-corrected & normalized) ---\n",
    "    chroma = librosa.feature.chroma_cqt(y=harmonic, sr=sr, tuning=tuning, hop_length=hop_length)\n",
    "    chroma = librosa.util.normalize(chroma, norm=1, axis=0)\n",
    "\n",
    "    # --- Beat-synchronous chroma aggregation ---\n",
    "    onset_env = librosa.onset.onset_strength(y=audio, sr=sr, hop_length=hop_length)\n",
    "    _, beats = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr, hop_length=hop_length)\n",
    "    if len(beats) > 0:\n",
    "        beat_chroma = librosa.util.sync(chroma, beats, aggregate=np.median)\n",
    "    else:\n",
    "        beat_chroma = chroma  # fallback if beat tracking fails\n",
    "    features[\"chroma_mean\"] = np.mean(beat_chroma, axis=1)\n",
    "    features[\"chroma_std\"] = np.std(beat_chroma, axis=1)\n",
    "    # --- Key-template correlation (Krumhansl & Kessler profiles) ---\n",
    "    krum_major = np.array([6.35, 2.23, 3.48, 2.33, 4.38, 4.09,\n",
    "                           2.52, 5.19, 2.39, 3.66, 2.29, 2.88])\n",
    "    krum_minor = np.array([6.33, 2.68, 3.52, 5.38, 2.60, 3.53,\n",
    "                           2.54, 4.75, 3.98, 2.69, 3.34, 3.17])\n",
    "    krum_major /= np.linalg.norm(krum_major)\n",
    "    krum_minor /= np.linalg.norm(krum_minor)\n",
    "    corrs = []\n",
    "    for k in range(12):\n",
    "        corrs.append(np.dot(features[\"chroma_mean\"], np.roll(krum_major, k)))\n",
    "    for k in range(12):\n",
    "        corrs.append(np.dot(features[\"chroma_mean\"], np.roll(krum_minor, k)))\n",
    "    features[\"key_template_corr\"] = np.array(corrs)\n",
    "    # --- Predominant pitch histogram ---\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "        harmonic,\n",
    "        fmin=librosa.note_to_hz(\"C2\"),\n",
    "        fmax=librosa.note_to_hz(\"C7\"),\n",
    "        sr=sr\n",
    "    )\n",
    "    if f0 is not None:\n",
    "        semitones = (12 * np.log2(f0 / 440.0) + 69) % 12\n",
    "        semitones = semitones[~np.isnan(semitones)]\n",
    "        hist, _ = np.histogram(semitones, bins=np.arange(13))\n",
    "        hist = hist / (np.sum(hist) + 1e-12)\n",
    "        features[\"pitch_hist\"] = hist\n",
    "    else:\n",
    "        features[\"pitch_hist\"] = np.zeros(12)\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Pitch class distribution\n",
    "    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    axes[0].bar(note_names, features[\"pitch_hist\"])\n",
    "    axes[0].set_xlabel(\"Pitch Class\")\n",
    "    axes[0].set_ylabel(\"Probability\")\n",
    "    axes[0].set_title(\"Pitch Class Distribution\")\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Key correlation (Major vs Minor)\n",
    "    major_corrs = features[\"key_template_corr\"][:12]\n",
    "    minor_corrs = features[\"key_template_corr\"][12:]\n",
    "    \n",
    "    key_labels = [f\"{note}\\nMaj\" for note in note_names] + [f\"{note}\\nMin\" for note in note_names]\n",
    "    colors = ['#3498db'] * 12 + ['#e74c3c'] * 12  # Blue for major, red for minor\n",
    "    \n",
    "    axes[1].bar(range(24), features[\"key_template_corr\"], color=colors, alpha=0.7)\n",
    "    axes[1].set_xlabel(\"Key\")\n",
    "    axes[1].set_ylabel(\"Correlation\")\n",
    "    axes[1].set_title(\"Krumhansl-Kessler Key Profile Correlation\")\n",
    "    axes[1].set_xticks(range(24))\n",
    "    axes[1].set_xticklabels(key_labels, rotation=45, ha='right', fontsize=8)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='#3498db', alpha=0.7, label='Major'),\n",
    "                      Patch(facecolor='#e74c3c', alpha=0.7, label='Minor')]\n",
    "    axes[1].legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # Find and annotate the most likely key\n",
    "    best_idx = np.argmax(features[\"key_template_corr\"])\n",
    "    best_key = key_labels[best_idx].replace('\\n', ' ')\n",
    "    axes[1].annotate(f'Most likely: {best_key}', \n",
    "                     xy=(best_idx, features[\"key_template_corr\"][best_idx]),\n",
    "                     xytext=(10, 10), textcoords='offset points',\n",
    "                     bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n",
    "                     arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return features\n",
    "\n",
    "features = extract_features(harmonic_audio, sr=44100)\n",
    "for k, v in features.items():\n",
    "    print(f\"{k}: {np.shape(v)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
